{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb8f82a",
   "metadata": {},
   "source": [
    "## BACEN: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cac3467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded service account credentials.\n",
      "\n",
      "--- Starting data extraction from BACEN API (2010-02-27 to 2023-01-15) ---\n",
      "Successfully fetched 253 records for period 02-27-2010 to 02-27-2011\n",
      "Successfully fetched 251 records for period 02-28-2011 to 02-28-2012\n",
      "Successfully fetched 251 records for period 02-29-2012 to 02-28-2013\n",
      "Successfully fetched 255 records for period 03-01-2013 to 03-01-2014\n",
      "Successfully fetched 251 records for period 03-02-2014 to 03-02-2015\n",
      "Successfully fetched 251 records for period 03-03-2015 to 03-02-2016\n",
      "Successfully fetched 253 records for period 03-03-2016 to 03-03-2017\n",
      "Successfully fetched 248 records for period 03-04-2017 to 03-04-2018\n",
      "Successfully fetched 251 records for period 03-05-2018 to 03-05-2019\n",
      "Successfully fetched 254 records for period 03-06-2019 to 03-05-2020\n",
      "Successfully fetched 250 records for period 03-06-2020 to 03-06-2021\n",
      "Successfully fetched 252 records for period 03-07-2021 to 03-07-2022\n",
      "Successfully fetched 217 records for period 03-08-2022 to 01-15-2023\n",
      "\n",
      "--- Preparing to upload 3237 rows to BigQuery table raw_external.bacen_dollar_exchange_rate... ---\n",
      "✅ Upload to BigQuery complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "from pandas_gbq import to_gbq\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Configuration ---\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# GCP project and table details\n",
    "PROJECT_ID = 'menu-engineering-466520'\n",
    "TABLE_ID = 'raw_external.bacen_dollar_exchange_rate'\n",
    "\n",
    "# Date range for the data extraction (based on our inspection)\n",
    "START_DATE = date(2010, 2, 27)\n",
    "END_DATE = date(2023, 1, 15)\n",
    "\n",
    "# The BACEN API is more stable with smaller date range chunks\n",
    "CHUNK_DAYS = 365\n",
    "\n",
    "# Explicitly load credentials from the JSON file specified in .env\n",
    "credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "try:\n",
    "    credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "    print(\"Successfully loaded service account credentials.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading credentials: {e}\")\n",
    "    credentials = None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if credentials:\n",
    "    all_data = []\n",
    "    current_start = START_DATE\n",
    "\n",
    "    print(f\"\\n--- Starting data extraction from BACEN API ({START_DATE} to {END_DATE}) ---\")\n",
    "    while current_start <= END_DATE:\n",
    "        current_end = current_start + timedelta(days=CHUNK_DAYS)\n",
    "        if current_end > END_DATE:\n",
    "            current_end = END_DATE\n",
    "\n",
    "        # Format dates for the API URL: MM-DD-YYYY\n",
    "        start_str = current_start.strftime('%m-%d-%Y')\n",
    "        end_str = current_end.strftime('%m-%d-%Y')\n",
    "\n",
    "        # Construct the API URL\n",
    "        url = f\"https://olinda.bcb.gov.br/olinda/servico/PTAX/versao/v1/odata/CotacaoDolarPeriodo(dataInicial=@dataInicial,dataFinalCotacao=@dataFinalCotacao)?@dataInicial='{start_str}'&@dataFinalCotacao='{end_str}'&$format=json\"\n",
    "        \n",
    "        try:\n",
    "            # Make the API request\n",
    "            response = requests.get(url, timeout=30) # Added a timeout for safety\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json().get('value', [])\n",
    "                if data:\n",
    "                    all_data.extend(data)\n",
    "                    print(f\"Successfully fetched {len(data)} records for period {start_str} to {end_str}\")\n",
    "                else:\n",
    "                    print(f\"No data returned for period {start_str} to {end_str}\")\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for period {start_str} to {end_str}. Status code: {response.status_code}\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred during API request for period {start_str} to {end_str}: {e}\")\n",
    "\n",
    "        # Move to the next period\n",
    "        current_start = current_end + timedelta(days=1)\n",
    "\n",
    "    # --- Process and Load Data to BigQuery ---\n",
    "    if all_data:\n",
    "        # Convert the collected JSON data to a Pandas DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "\n",
    "        # Select and rename relevant columns for clarity\n",
    "        df = df[['cotacaoVenda', 'dataHoraCotacao']]\n",
    "        df.rename(columns={\n",
    "            'cotacaoVenda': 'usd_exchange_rate_sell',\n",
    "            'dataHoraCotacao': 'exchange_date_time'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Convert columns to the correct data types\n",
    "        df['usd_exchange_rate_sell'] = pd.to_numeric(df['usd_exchange_rate_sell'])\n",
    "        df['exchange_date_time'] = pd.to_datetime(df['exchange_date_time'], utc=True)\n",
    "        \n",
    "        # Create a pure DATE column for easier joins\n",
    "        df['exchange_date'] = df['exchange_date_time'].dt.date\n",
    "\n",
    "        print(f\"\\n--- Preparing to upload {len(df)} rows to BigQuery table {TABLE_ID}... ---\")\n",
    "        \n",
    "        try:\n",
    "            # Load the DataFrame to BigQuery\n",
    "            to_gbq(\n",
    "                df,\n",
    "                destination_table=TABLE_ID,\n",
    "                project_id=PROJECT_ID,\n",
    "                credentials=credentials,\n",
    "                if_exists='replace' # Use 'replace' to overwrite the table on each run, making the script rerunnable\n",
    "            )\n",
    "            print(\"✅ Upload to BigQuery complete!\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to upload data to BigQuery: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\nNo data was fetched from the API. Nothing to upload.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
